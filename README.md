# ğŸ”¥ Transformer from Scratch: "Attention is All You Need" (NIPS 2017)

This repository contains a complete ğŸ’¡ from-scratch implementation of the **Transformer** architecture as described in the groundbreaking paper:

> ğŸ“œ **"Attention is All You Need"**  
> âœï¸ Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, Illia Polosukhin  
> ğŸ§  NeurIPS 2017  
> [ğŸ“„ View Original Paper (PDF)](./paper/NIPS-2017-attention-is-all-you-need-Paper.pdf)

---

## ğŸ” Overview

The Transformer model revolutionized deep learning in sequence modeling by **removing recurrence altogether** and using only attention mechanisms â€” enabling âš¡faster training, ğŸ’¯ better performance, and ğŸ§± cleaner architecture.

This implementation includes:
- ğŸ—ï¸ Encoder-Decoder structure
- ğŸ§® Scaled Dot-Product Attention
- ğŸ¯ Multi-Head Attention
- ğŸ§­ Sinusoidal Positional Encoding
- âš™ï¸ Position-wise Feedforward Networks
- ğŸ§± Residual Connections + Layer Normalization
- ğŸ”’ Masked decoding to preserve autoregression

---

## ğŸ§  Core Concepts

| Concept ğŸ”‘                      | Description ğŸ“˜                                                                 |
|-------------------------------|--------------------------------------------------------------------------------|
| ğŸ’¥ Scaled Dot-Product Attention | Calculates attention weights using dot products (scaled by âˆšdimension)        |
| ğŸ§  Multi-Head Attention         | Enables parallel attention over multiple subspaces                            |
| ğŸ§­ Positional Encoding          | Injects order-awareness using sin/cos positional signals                      |
| ğŸ§± Encoder-Decoder              | Maps input â†’ latent â†’ output via attention and feedforward layers            |
| ğŸ•¶ï¸ Masked Attention             | Prevents the decoder from seeing future tokens during training               |

---




